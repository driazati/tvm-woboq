<def f='tvm/relay/qnn/op/batch_matmul.cc' l='162' ll='201' type='tvm::relay::Expr tvm::relay::qnn::QnnBatchMatmulCanonicalize(const tvm::Attrs &amp; attrs, const Array&lt;tvm::relay::Expr&gt; &amp; new_args, const Array&lt;tvm::relay::Type&gt; &amp; arg_types)'/>
<use f='tvm/relay/qnn/op/batch_matmul.cc' l='228' u='a'/>
<doc f='tvm/relay/qnn/op/batch_matmul.cc' l='127'>/*
 * \brief Forward rewrite the qnn batch_matmul op.
 * \param attrs The QNN batch_matmul attrs.
 * \param new_args The new mutated args to the call node.
 * \param arg_types The types of input and output.
 * \return The sequence of Relay ops for qnn batch_matmul op.
 * \note Lowering of the qnn.batch_matmul operator
 *       A quantized tensor is represented in following manner
 *          A = scale_a x (QA - zp_A)
 *       where QA is quantized tensor, scale_a and zp_A are quantization
 *       params.
 *
 *       Quantized batch_matmul multiplies two quantized tensors and returns a
 *       quantized tensor of default dtype of int32, with scale equaling to the
 *       product of scales of input tensors, and a zero point of zero.
 *
 *       The lowering for asymmetric quantized batch_matmul looks similar to
 *       quantized conv2d and dense and originally was discussed here:
 *       https://discuss.tvm.apache.org/t/tf-lite-quantized-conv2d-operator-conversion/2651/7
 *
 *       The computation gets unrolled into following 4 terms
 *          C(m, n) = Sigma(k) (X(m, k) * Y(n, k))
 *
 *          RHS becomes
 *            Sigma(k) ([QX(m, k) - zp_x] * [QY(n, k) - zp_y])
 *
 *          Unrolling leads to following sequence
 *            Sigma(k) QX(m, k) * QX(n, k)                         // Term1
 *          - Sigma(k) zp_y * QX(m, k)                             // Term2
 *          - Sigma(k) zp_x * QY(n, k)                             // Term3
 *          - Sigma(k) * zp_x * zp_y                               // Term4
 *
 *       Term4 can be computed at compile time, everything else depending on the
 *       input type.
 */</doc>
